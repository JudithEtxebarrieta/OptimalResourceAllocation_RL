[2025-02-11 08:22:59,608][07529] Saving configuration to experiments_LibrariesRL/results/samplefactory/execution1/config.json...
[2025-02-11 08:22:59,672][07529] Rollout worker 0 uses device cpu
[2025-02-11 08:22:59,672][07529] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
[2025-02-11 08:22:59,695][07529] InferenceWorker_p0-w0: min num requests: 1
[2025-02-11 08:22:59,697][07529] Starting all processes...
[2025-02-11 08:22:59,697][07529] Starting process learner_proc0
[2025-02-11 08:22:59,747][07529] Starting all processes...
[2025-02-11 08:22:59,749][07529] Starting process inference_proc0-0
[2025-02-11 08:22:59,749][07529] Starting process rollout_proc0
[2025-02-11 08:23:01,584][07580] Setting fixed seed 1
[2025-02-11 08:23:01,586][07580] Initializing actor-critic model on device cpu
[2025-02-11 08:23:01,586][07580] RunningMeanStd input shape: (27,)
[2025-02-11 08:23:01,587][07580] RunningMeanStd input shape: (1,)
[2025-02-11 08:23:01,628][07587] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-02-11 08:23:01,639][07580] Created Actor Critic model with architecture:
[2025-02-11 08:23:01,639][07580] ActorCriticSharedWeights(
  (obs_normalizer): ObservationNormalizer(
    (running_mean_std): RunningMeanStdDictInPlace(
      (running_mean_std): ModuleDict(
        (obs): RunningMeanStdInPlace()
      )
    )
  )
  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
  (encoder): MultiInputEncoder(
    (encoders): ModuleDict(
      (obs): MlpEncoder(
        (mlp_head): RecursiveScriptModule(
          original_name=Sequential
          (0): RecursiveScriptModule(original_name=Linear)
          (1): RecursiveScriptModule(original_name=Tanh)
          (2): RecursiveScriptModule(original_name=Linear)
          (3): RecursiveScriptModule(original_name=Tanh)
        )
      )
    )
  )
  (core): ModelCoreIdentity()
  (decoder): MlpDecoder(
    (mlp): Identity()
  )
  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
  (action_parameterization): ActionParameterizationContinuousNonAdaptiveStddev(
    (distribution_linear): Linear(in_features=64, out_features=8, bias=True)
  )
)
[2025-02-11 08:23:01,854][07580] Using optimizer <class 'torch.optim.adam.Adam'>
[2025-02-11 08:23:02,660][07580] No checkpoints found
[2025-02-11 08:23:02,660][07580] Did not load from checkpoint, starting from scratch!
[2025-02-11 08:23:02,661][07580] Initialized policy 0 weights for model version 0
[2025-02-11 08:23:02,662][07580] LearnerWorker_p0 finished initialization!
[2025-02-11 08:23:02,663][07586] RunningMeanStd input shape: (27,)
[2025-02-11 08:23:02,663][07586] RunningMeanStd input shape: (1,)
[2025-02-11 08:23:02,705][07529] Inference worker 0-0 is ready!
[2025-02-11 08:23:02,705][07529] All inference workers are ready! Signal rollout workers to start!
[2025-02-11 08:23:02,769][07587] Decorrelating experience for 0 frames...
[2025-02-11 08:23:03,930][07587] Stopping RolloutWorker_w0...
[2025-02-11 08:23:03,930][07529] Component RolloutWorker_w0 stopped!
[2025-02-11 08:23:03,930][07587] Loop rollout_proc0_evt_loop terminating...
[2025-02-11 08:23:03,931][07580] Stopping Batcher_0...
[2025-02-11 08:23:03,931][07529] Component Batcher_0 stopped!
[2025-02-11 08:23:03,931][07580] Loop batcher_evt_loop terminating...
[2025-02-11 08:23:03,932][07580] Saving experiments_LibrariesRL/results/samplefactory/execution1/checkpoint_p0/checkpoint_000000007_896.pth...
[2025-02-11 08:23:03,934][07580] Saving experiments_LibrariesRL/results/samplefactory/execution1/checkpoint_p0/checkpoint_000000007_896.pth...
[2025-02-11 08:23:03,936][07580] Stopping LearnerWorker_p0...
[2025-02-11 08:23:03,936][07580] Loop learner_proc0_evt_loop terminating...
[2025-02-11 08:23:03,936][07529] Component LearnerWorker_p0 stopped!
[2025-02-11 08:23:03,975][07586] Weights refcount: 2 0
[2025-02-11 08:23:03,976][07586] Stopping InferenceWorker_p0-w0...
[2025-02-11 08:23:03,976][07586] Loop inference_proc0-0_evt_loop terminating...
[2025-02-11 08:23:03,977][07529] Component InferenceWorker_p0-w0 stopped!
[2025-02-11 08:23:03,978][07529] Waiting for process learner_proc0 to stop...
[2025-02-11 08:23:04,569][07529] Waiting for process inference_proc0-0 to join...
[2025-02-11 08:23:04,569][07529] Waiting for process rollout_proc0 to join...
[2025-02-11 08:23:04,570][07529] Batcher 0 profile tree view:
batching: 0.0027, releasing_batches: 0.0008
[2025-02-11 08:23:04,570][07529] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0051
  wait_policy_total: 0.6793
update_model: 0.0184
  weight_update: 0.0003
one_step: 0.0020
  handle_policy_step: 0.4885
    deserialize: 0.0206, stack: 0.0060, obs_to_device_normalize: 0.1043, forward: 0.2432, send_messages: 0.0281
    prepare_outputs: 0.0504
      to_cpu: 0.0072
[2025-02-11 08:23:04,570][07529] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.0368
train: 0.0355
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0003, kl_divergence: 0.0001, after_optimizer: 0.0010
  calculate_losses: 0.0108
    losses_init: 0.0000, forward_head: 0.0036, bptt_initial: 0.0000, bptt: 0.0000, tail: 0.0035, advantages_returns: 0.0005, losses: 0.0028
  update: 0.0221
    clip: 0.0029
[2025-02-11 08:23:04,570][07529] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0005, enqueue_policy_requests: 0.0331, env_step: 0.2496, overhead: 0.0205, complete_rollouts: 0.0009
save_policy_outputs: 0.0469
  split_output_tensors: 0.0166
[2025-02-11 08:23:04,570][07529] Loop Runner_EvtLoop terminating...
[2025-02-11 08:23:04,571][07529] Runner profile tree view:
main_loop: 4.8737
[2025-02-11 08:23:04,571][07529] Collected {0: 896}, FPS: 183.8
[2025-02-11 08:23:04,571][07529] Environment mujoco_hopper already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_halfcheetah already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_humanoid already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_ant already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_standup already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_doublependulum already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_pendulum already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_reacher already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_walker already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_pusher already registered, overwriting...
[2025-02-11 08:23:04,571][07529] Environment mujoco_swimmer already registered, overwriting...
[2025-02-11 08:23:04,577][07529] Saved parameter configuration for experiment execution2 not found!
[2025-02-11 08:23:04,577][07529] Starting experiment from scratch!
[2025-02-11 08:23:04,580][07529] Experiment dir experiments_LibrariesRL/results/samplefactory/execution2 already exists!
[2025-02-11 08:23:04,580][07529] Resuming existing experiment from experiments_LibrariesRL/results/samplefactory/execution2...
[2025-02-11 08:23:04,580][07529] Weights and Biases integration disabled
[2025-02-11 08:23:04,581][07529] Environment var CUDA_VISIBLE_DEVICES is 
[2025-02-11 08:23:06,425][07529] Starting experiment with the following configuration:
help=False
algo=APPO
env=mujoco_ant
experiment=execution2
train_dir=experiments_LibrariesRL/results/samplefactory
restart_behavior=resume
device=cpu
seed=1
num_policies=1
async_rl=False
serial_mode=False
batched_sampling=False
num_batches_to_accumulate=2
worker_num_splits=1
policy_workers_per_policy=1
max_policy_lag=1000
num_workers=1
num_envs_per_worker=1
batch_size=128
num_batches_per_epoch=1
num_epochs=1
rollout=64
recurrence=1
shuffle_minibatches=False
gamma=0.99
reward_scale=1
reward_clip=1000.0
value_bootstrap=True
normalize_returns=True
exploration_loss_coeff=0.0
value_loss_coeff=1.3
kl_loss_coeff=0.1
exploration_loss=entropy
gae_lambda=0.95
ppo_clip_ratio=0.2
ppo_clip_value=1.0
with_vtrace=False
vtrace_rho=1.0
vtrace_c=1.0
optimizer=adam
adam_eps=1e-06
adam_beta1=0.9
adam_beta2=0.999
max_grad_norm=3.5
learning_rate=0.00295
lr_schedule=linear_decay
lr_schedule_kl_threshold=0.008
lr_adaptive_min=1e-06
lr_adaptive_max=0.01
obs_subtract_mean=0.0
obs_scale=1.0
normalize_input=True
normalize_input_keys=None
decorrelate_experience_max_seconds=0
decorrelate_envs_on_one_worker=True
actor_worker_gpus=[]
set_workers_cpu_affinity=True
force_envs_single_thread=False
default_niceness=0
log_to_file=True
experiment_summaries_interval=3
flush_summaries_interval=30
stats_avg=100
summaries_use_frameskip=True
heartbeat_interval=20
heartbeat_reporting_interval=180
train_for_env_steps=640
train_for_seconds=10000000000
save_every_sec=15
keep_checkpoints=2
load_checkpoint_kind=latest
save_milestones_sec=-1
save_best_every_sec=5
save_best_metric=reward
save_best_after=100000
benchmark=False
encoder_mlp_layers=[64, 64]
encoder_conv_architecture=convnet_simple
encoder_conv_mlp_layers=[512]
use_rnn=False
rnn_size=512
rnn_type=gru
rnn_num_layers=1
decoder_mlp_layers=[]
nonlinearity=tanh
policy_initialization=torch_default
policy_init_gain=1.0
actor_critic_share_weights=True
adaptive_stddev=False
continuous_tanh_scale=0.0
initial_stddev=1.0
use_env_info_cache=False
env_gpu_actions=False
env_gpu_observations=True
env_frameskip=1
env_framestack=1
pixel_format=CHW
use_record_episode_statistics=False
episode_counter=False
with_wandb=False
wandb_user=None
wandb_project=sample_factory
wandb_group=None
wandb_job_type=SF
wandb_tags=[]
with_pbt=False
pbt_mix_policies_in_one_env=True
pbt_period_env_steps=5000000
pbt_start_mutation=20000000
pbt_replace_fraction=0.3
pbt_mutation_rate=0.15
pbt_replace_reward_gap=0.1
pbt_replace_reward_gap_absolute=1e-06
pbt_optimize_gamma=False
pbt_target_objective=true_objective
pbt_perturb_min=1.1
pbt_perturb_max=1.5
command_line=--algo=APPO --env=mujoco_ant --seed=1 --train_for_env_steps=640 --experiment=execution2 --train_dir=experiments_LibrariesRL/results/samplefactory --device=cpu --rollout=64 --num_workers=1 --num_envs_per_worker=1 --worker_num_splits=1 --batch_size=128 --num_batches_per_epoch=1 --num_epoch=1 --save_every_sec=15 --keep_checkpoints=2 --save_best_every_sec=5 --save_best_metric=reward --save_best_after=100000 --stats_avg=100
cli_args={'algo': 'APPO', 'env': 'mujoco_ant', 'experiment': 'execution2', 'train_dir': 'experiments_LibrariesRL/results/samplefactory', 'device': 'cpu', 'seed': 1, 'worker_num_splits': 1, 'num_workers': 1, 'num_envs_per_worker': 1, 'batch_size': 128, 'num_batches_per_epoch': 1, 'num_epochs': 1, 'rollout': 64, 'stats_avg': 100, 'train_for_env_steps': 640, 'save_every_sec': 15, 'keep_checkpoints': 2, 'save_best_every_sec': 5, 'save_best_metric': 'reward', 'save_best_after': 100000}
git_hash=4df1bb5bc37eecd8a442a21f0e1a6db08ba9ffe2
git_repo_name=https://github.com/JudithEtxebarrieta/OptimalResourceAllocation_RL.git
[2025-02-11 08:23:06,425][07529] Saving configuration to experiments_LibrariesRL/results/samplefactory/execution2/config.json...
[2025-02-11 08:23:06,490][07529] Rollout worker 0 uses device cpu
[2025-02-11 08:23:06,490][07529] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
[2025-02-11 08:23:06,499][07529] InferenceWorker_p0-w0: min num requests: 1
[2025-02-11 08:23:06,501][07529] Starting all processes...
[2025-02-11 08:23:06,501][07529] Starting process learner_proc0
[2025-02-11 08:23:06,551][07529] Starting all processes...
[2025-02-11 08:23:06,552][07529] Starting process inference_proc0-0
[2025-02-11 08:23:06,553][07529] Starting process rollout_proc0
[2025-02-11 08:23:09,097][07529] Inference worker 0-0 is ready!
[2025-02-11 08:23:09,097][07529] All inference workers are ready! Signal rollout workers to start!
[2025-02-11 08:23:09,581][07529] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 256. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[2025-02-11 08:23:09,582][07529] Avg episode reward: [(0, '-127.540')]
[2025-02-11 08:23:10,170][07529] Component Batcher_0 stopped!
[2025-02-11 08:23:10,171][07529] Component RolloutWorker_w0 stopped!
[2025-02-11 08:23:10,177][07529] Component LearnerWorker_p0 stopped!
[2025-02-11 08:23:10,214][07529] Component InferenceWorker_p0-w0 stopped!
[2025-02-11 08:23:10,214][07529] Waiting for process learner_proc0 to stop...
[2025-02-11 08:23:10,807][07529] Waiting for process inference_proc0-0 to join...
[2025-02-11 08:23:10,807][07529] Waiting for process rollout_proc0 to join...
[2025-02-11 08:23:10,807][07529] Batcher 0 profile tree view:
batching: 0.0025, releasing_batches: 0.0007
[2025-02-11 08:23:10,808][07529] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0051
  wait_policy_total: 0.5605
update_model: 0.0169
  weight_update: 0.0003
one_step: 0.0008
  handle_policy_step: 0.4585
    deserialize: 0.0195, stack: 0.0055, obs_to_device_normalize: 0.0960, forward: 0.2270, send_messages: 0.0284
    prepare_outputs: 0.0474
      to_cpu: 0.0065
[2025-02-11 08:23:10,808][07529] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.0427
train: 0.0297
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0003, kl_divergence: 0.0001, after_optimizer: 0.0007
  calculate_losses: 0.0092
    losses_init: 0.0000, forward_head: 0.0017, bptt_initial: 0.0000, bptt: 0.0000, tail: 0.0034, advantages_returns: 0.0006, losses: 0.0031
  update: 0.0181
    clip: 0.0014
[2025-02-11 08:23:10,808][07529] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0006, enqueue_policy_requests: 0.0311, env_step: 0.2341, overhead: 0.0193, complete_rollouts: 0.0009
save_policy_outputs: 0.0438
  split_output_tensors: 0.0162
[2025-02-11 08:23:10,808][07529] Loop Runner_EvtLoop terminating...
[2025-02-11 08:23:10,808][07529] Runner profile tree view:
main_loop: 4.3074
[2025-02-11 08:23:10,808][07529] Collected {0: 896}, FPS: 208.0
[2025-02-11 08:23:10,809][07529] Environment mujoco_hopper already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_halfcheetah already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_humanoid already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_ant already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_standup already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_doublependulum already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_pendulum already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_reacher already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_walker already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_pusher already registered, overwriting...
[2025-02-11 08:23:10,809][07529] Environment mujoco_swimmer already registered, overwriting...
[2025-02-11 08:23:10,815][07529] Saved parameter configuration for experiment execution3 not found!
[2025-02-11 08:23:10,815][07529] Starting experiment from scratch!
[2025-02-11 08:23:10,819][07529] Experiment dir experiments_LibrariesRL/results/samplefactory/execution3 already exists!
[2025-02-11 08:23:10,819][07529] Resuming existing experiment from experiments_LibrariesRL/results/samplefactory/execution3...
[2025-02-11 08:23:10,819][07529] Weights and Biases integration disabled
[2025-02-11 08:23:10,820][07529] Environment var CUDA_VISIBLE_DEVICES is 
[2025-02-11 08:23:12,715][07529] Starting experiment with the following configuration:
help=False
algo=APPO
env=mujoco_ant
experiment=execution3
train_dir=experiments_LibrariesRL/results/samplefactory
restart_behavior=resume
device=cpu
seed=1
num_policies=1
async_rl=False
serial_mode=False
batched_sampling=False
num_batches_to_accumulate=2
worker_num_splits=1
policy_workers_per_policy=1
max_policy_lag=1000
num_workers=2
num_envs_per_worker=1
batch_size=128
num_batches_per_epoch=1
num_epochs=1
rollout=64
recurrence=1
shuffle_minibatches=False
gamma=0.99
reward_scale=1
reward_clip=1000.0
value_bootstrap=True
normalize_returns=True
exploration_loss_coeff=0.0
value_loss_coeff=1.3
kl_loss_coeff=0.1
exploration_loss=entropy
gae_lambda=0.95
ppo_clip_ratio=0.2
ppo_clip_value=1.0
with_vtrace=False
vtrace_rho=1.0
vtrace_c=1.0
optimizer=adam
adam_eps=1e-06
adam_beta1=0.9
adam_beta2=0.999
max_grad_norm=3.5
learning_rate=0.00295
lr_schedule=linear_decay
lr_schedule_kl_threshold=0.008
lr_adaptive_min=1e-06
lr_adaptive_max=0.01
obs_subtract_mean=0.0
obs_scale=1.0
normalize_input=True
normalize_input_keys=None
decorrelate_experience_max_seconds=0
decorrelate_envs_on_one_worker=True
actor_worker_gpus=[]
set_workers_cpu_affinity=True
force_envs_single_thread=False
default_niceness=0
log_to_file=True
experiment_summaries_interval=3
flush_summaries_interval=30
stats_avg=100
summaries_use_frameskip=True
heartbeat_interval=20
heartbeat_reporting_interval=180
train_for_env_steps=640
train_for_seconds=10000000000
save_every_sec=15
keep_checkpoints=2
load_checkpoint_kind=latest
save_milestones_sec=-1
save_best_every_sec=5
save_best_metric=reward
save_best_after=100000
benchmark=False
encoder_mlp_layers=[64, 64]
encoder_conv_architecture=convnet_simple
encoder_conv_mlp_layers=[512]
use_rnn=False
rnn_size=512
rnn_type=gru
rnn_num_layers=1
decoder_mlp_layers=[]
nonlinearity=tanh
policy_initialization=torch_default
policy_init_gain=1.0
actor_critic_share_weights=True
adaptive_stddev=False
continuous_tanh_scale=0.0
initial_stddev=1.0
use_env_info_cache=False
env_gpu_actions=False
env_gpu_observations=True
env_frameskip=1
env_framestack=1
pixel_format=CHW
use_record_episode_statistics=False
episode_counter=False
with_wandb=False
wandb_user=None
wandb_project=sample_factory
wandb_group=None
wandb_job_type=SF
wandb_tags=[]
with_pbt=False
pbt_mix_policies_in_one_env=True
pbt_period_env_steps=5000000
pbt_start_mutation=20000000
pbt_replace_fraction=0.3
pbt_mutation_rate=0.15
pbt_replace_reward_gap=0.1
pbt_replace_reward_gap_absolute=1e-06
pbt_optimize_gamma=False
pbt_target_objective=true_objective
pbt_perturb_min=1.1
pbt_perturb_max=1.5
command_line=--algo=APPO --env=mujoco_ant --seed=1 --train_for_env_steps=640 --experiment=execution3 --train_dir=experiments_LibrariesRL/results/samplefactory --device=cpu --rollout=64 --num_workers=2 --num_envs_per_worker=1 --worker_num_splits=1 --batch_size=128 --num_batches_per_epoch=1 --num_epoch=1 --save_every_sec=15 --keep_checkpoints=2 --save_best_every_sec=5 --save_best_metric=reward --save_best_after=100000 --stats_avg=100
cli_args={'algo': 'APPO', 'env': 'mujoco_ant', 'experiment': 'execution3', 'train_dir': 'experiments_LibrariesRL/results/samplefactory', 'device': 'cpu', 'seed': 1, 'worker_num_splits': 1, 'num_workers': 2, 'num_envs_per_worker': 1, 'batch_size': 128, 'num_batches_per_epoch': 1, 'num_epochs': 1, 'rollout': 64, 'stats_avg': 100, 'train_for_env_steps': 640, 'save_every_sec': 15, 'keep_checkpoints': 2, 'save_best_every_sec': 5, 'save_best_metric': 'reward', 'save_best_after': 100000}
git_hash=4df1bb5bc37eecd8a442a21f0e1a6db08ba9ffe2
git_repo_name=https://github.com/JudithEtxebarrieta/OptimalResourceAllocation_RL.git
[2025-02-11 08:23:12,716][07529] Saving configuration to experiments_LibrariesRL/results/samplefactory/execution3/config.json...
[2025-02-11 08:23:12,780][07529] Rollout worker 0 uses device cpu
[2025-02-11 08:23:12,781][07529] Rollout worker 1 uses device cpu
[2025-02-11 08:23:12,781][07529] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
[2025-02-11 08:23:12,794][07529] InferenceWorker_p0-w0: min num requests: 1
[2025-02-11 08:23:12,800][07529] Starting all processes...
[2025-02-11 08:23:12,800][07529] Starting process learner_proc0
[2025-02-11 08:23:12,850][07529] Starting all processes...
[2025-02-11 08:23:12,856][07529] Starting process inference_proc0-0
[2025-02-11 08:23:12,856][07529] Starting process rollout_proc0
[2025-02-11 08:23:12,856][07529] Starting process rollout_proc1
[2025-02-11 08:23:15,555][07529] Inference worker 0-0 is ready!
[2025-02-11 08:23:15,555][07529] All inference workers are ready! Signal rollout workers to start!
[2025-02-11 08:23:15,820][07529] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 128. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[2025-02-11 08:23:16,348][07529] Component Batcher_0 stopped!
[2025-02-11 08:23:16,348][07529] Component RolloutWorker_w0 stopped!
[2025-02-11 08:23:16,349][07529] Component RolloutWorker_w1 stopped!
[2025-02-11 08:23:16,362][07529] Component LearnerWorker_p0 stopped!
[2025-02-11 08:23:16,400][07529] Component InferenceWorker_p0-w0 stopped!
[2025-02-11 08:23:16,401][07529] Waiting for process learner_proc0 to stop...
[2025-02-11 08:23:17,031][07529] Waiting for process inference_proc0-0 to join...
[2025-02-11 08:23:17,032][07529] Waiting for process rollout_proc0 to join...
[2025-02-11 08:23:17,032][07529] Waiting for process rollout_proc1 to join...
[2025-02-11 08:23:17,032][07529] Batcher 0 profile tree view:
batching: 0.0025, releasing_batches: 0.0018
[2025-02-11 08:23:17,032][07529] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0051
  wait_policy_total: 0.3056
update_model: 0.0141
  weight_update: 0.0004
one_step: 0.0022
  handle_policy_step: 0.4433
    deserialize: 0.0164, stack: 0.0050, obs_to_device_normalize: 0.0894, forward: 0.2253, send_messages: 0.0261
    prepare_outputs: 0.0482
      to_cpu: 0.0070
[2025-02-11 08:23:17,032][07529] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.0339
train: 0.0292
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0003, kl_divergence: 0.0001, after_optimizer: 0.0006
  calculate_losses: 0.0081
    losses_init: 0.0000, forward_head: 0.0016, bptt_initial: 0.0000, bptt: 0.0000, tail: 0.0032, advantages_returns: 0.0004, losses: 0.0025
  update: 0.0188
    clip: 0.0016
[2025-02-11 08:23:17,033][07529] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0003, enqueue_policy_requests: 0.0161, env_step: 0.1201, overhead: 0.0097, complete_rollouts: 0.0004
save_policy_outputs: 0.0221
  split_output_tensors: 0.0078
[2025-02-11 08:23:17,033][07529] RolloutWorker_w1 profile tree view:
wait_for_trajectories: 0.0004, enqueue_policy_requests: 0.0228, env_step: 0.1698, overhead: 0.0145, complete_rollouts: 0.0006
save_policy_outputs: 0.0309
  split_output_tensors: 0.0102
[2025-02-11 08:23:17,033][07529] Loop Runner_EvtLoop terminating...
[2025-02-11 08:23:17,033][07529] Runner profile tree view:
main_loop: 4.2331
[2025-02-11 08:23:17,033][07529] Collected {0: 896}, FPS: 211.7
[2025-02-11 08:23:17,033][07529] Environment mujoco_hopper already registered, overwriting...
[2025-02-11 08:23:17,033][07529] Environment mujoco_halfcheetah already registered, overwriting...
[2025-02-11 08:23:17,033][07529] Environment mujoco_humanoid already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_ant already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_standup already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_doublependulum already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_pendulum already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_reacher already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_walker already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_pusher already registered, overwriting...
[2025-02-11 08:23:17,034][07529] Environment mujoco_swimmer already registered, overwriting...
[2025-02-11 08:23:17,045][07529] Saved parameter configuration for experiment execution4 not found!
[2025-02-11 08:23:17,046][07529] Starting experiment from scratch!
[2025-02-11 08:23:17,046][07529] Experiment dir experiments_LibrariesRL/results/samplefactory/execution4 already exists!
[2025-02-11 08:23:17,046][07529] Resuming existing experiment from experiments_LibrariesRL/results/samplefactory/execution4...
[2025-02-11 08:23:17,047][07529] Weights and Biases integration disabled
[2025-02-11 08:23:17,048][07529] Environment var CUDA_VISIBLE_DEVICES is 
[2025-02-11 08:23:19,069][07529] Starting experiment with the following configuration:
help=False
algo=APPO
env=mujoco_ant
experiment=execution4
train_dir=experiments_LibrariesRL/results/samplefactory
restart_behavior=resume
device=cpu
seed=1
num_policies=1
async_rl=False
serial_mode=False
batched_sampling=False
num_batches_to_accumulate=2
worker_num_splits=1
policy_workers_per_policy=1
max_policy_lag=1000
num_workers=2
num_envs_per_worker=1
batch_size=128
num_batches_per_epoch=1
num_epochs=1
rollout=64
recurrence=1
shuffle_minibatches=False
gamma=0.99
reward_scale=1
reward_clip=1000.0
value_bootstrap=True
normalize_returns=True
exploration_loss_coeff=0.0
value_loss_coeff=1.3
kl_loss_coeff=0.1
exploration_loss=entropy
gae_lambda=0.95
ppo_clip_ratio=0.2
ppo_clip_value=1.0
with_vtrace=False
vtrace_rho=1.0
vtrace_c=1.0
optimizer=adam
adam_eps=1e-06
adam_beta1=0.9
adam_beta2=0.999
max_grad_norm=3.5
learning_rate=0.00295
lr_schedule=linear_decay
lr_schedule_kl_threshold=0.008
lr_adaptive_min=1e-06
lr_adaptive_max=0.01
obs_subtract_mean=0.0
obs_scale=1.0
normalize_input=True
normalize_input_keys=None
decorrelate_experience_max_seconds=0
decorrelate_envs_on_one_worker=True
actor_worker_gpus=[]
set_workers_cpu_affinity=True
force_envs_single_thread=False
default_niceness=0
log_to_file=True
experiment_summaries_interval=3
flush_summaries_interval=30
stats_avg=100
summaries_use_frameskip=True
heartbeat_interval=20
heartbeat_reporting_interval=180
train_for_env_steps=640
train_for_seconds=10000000000
save_every_sec=15
keep_checkpoints=2
load_checkpoint_kind=latest
save_milestones_sec=-1
save_best_every_sec=5
save_best_metric=reward
save_best_after=100000
benchmark=False
encoder_mlp_layers=[64, 64]
encoder_conv_architecture=convnet_simple
encoder_conv_mlp_layers=[512]
use_rnn=False
rnn_size=512
rnn_type=gru
rnn_num_layers=1
decoder_mlp_layers=[]
nonlinearity=tanh
policy_initialization=torch_default
policy_init_gain=1.0
actor_critic_share_weights=True
adaptive_stddev=False
continuous_tanh_scale=0.0
initial_stddev=1.0
use_env_info_cache=False
env_gpu_actions=False
env_gpu_observations=True
env_frameskip=1
env_framestack=1
pixel_format=CHW
use_record_episode_statistics=False
episode_counter=False
with_wandb=False
wandb_user=None
wandb_project=sample_factory
wandb_group=None
wandb_job_type=SF
wandb_tags=[]
with_pbt=False
pbt_mix_policies_in_one_env=True
pbt_period_env_steps=5000000
pbt_start_mutation=20000000
pbt_replace_fraction=0.3
pbt_mutation_rate=0.15
pbt_replace_reward_gap=0.1
pbt_replace_reward_gap_absolute=1e-06
pbt_optimize_gamma=False
pbt_target_objective=true_objective
pbt_perturb_min=1.1
pbt_perturb_max=1.5
command_line=--algo=APPO --env=mujoco_ant --seed=1 --train_for_env_steps=640 --experiment=execution4 --train_dir=experiments_LibrariesRL/results/samplefactory --device=cpu --rollout=64 --num_workers=2 --num_envs_per_worker=1 --worker_num_splits=1 --batch_size=128 --num_batches_per_epoch=1 --num_epoch=1 --save_every_sec=15 --keep_checkpoints=2 --save_best_every_sec=5 --save_best_metric=reward --save_best_after=100000 --stats_avg=100
cli_args={'algo': 'APPO', 'env': 'mujoco_ant', 'experiment': 'execution4', 'train_dir': 'experiments_LibrariesRL/results/samplefactory', 'device': 'cpu', 'seed': 1, 'worker_num_splits': 1, 'num_workers': 2, 'num_envs_per_worker': 1, 'batch_size': 128, 'num_batches_per_epoch': 1, 'num_epochs': 1, 'rollout': 64, 'stats_avg': 100, 'train_for_env_steps': 640, 'save_every_sec': 15, 'keep_checkpoints': 2, 'save_best_every_sec': 5, 'save_best_metric': 'reward', 'save_best_after': 100000}
git_hash=4df1bb5bc37eecd8a442a21f0e1a6db08ba9ffe2
git_repo_name=https://github.com/JudithEtxebarrieta/OptimalResourceAllocation_RL.git
[2025-02-11 08:23:19,069][07529] Saving configuration to experiments_LibrariesRL/results/samplefactory/execution4/config.json...
[2025-02-11 08:23:19,134][07529] Rollout worker 0 uses device cpu
[2025-02-11 08:23:19,134][07529] Rollout worker 1 uses device cpu
[2025-02-11 08:23:19,134][07529] In synchronous mode, we only accumulate one batch. Setting num_batches_to_accumulate to 1
[2025-02-11 08:23:19,141][07529] InferenceWorker_p0-w0: min num requests: 1
[2025-02-11 08:23:19,145][07529] Starting all processes...
[2025-02-11 08:23:19,145][07529] Starting process learner_proc0
[2025-02-11 08:23:19,195][07529] Starting all processes...
[2025-02-11 08:23:19,199][07529] Starting process inference_proc0-0
[2025-02-11 08:23:19,199][07529] Starting process rollout_proc0
[2025-02-11 08:23:19,199][07529] Starting process rollout_proc1
[2025-02-11 08:23:22,624][07529] Inference worker 0-0 is ready!
[2025-02-11 08:23:22,624][07529] All inference workers are ready! Signal rollout workers to start!
[2025-02-11 08:23:23,499][07529] Component RolloutWorker_w0 stopped!
[2025-02-11 08:23:23,499][07529] Component RolloutWorker_w1 stopped!
[2025-02-11 08:23:23,500][07529] Component Batcher_0 stopped!
[2025-02-11 08:23:23,507][07529] Component LearnerWorker_p0 stopped!
[2025-02-11 08:23:23,547][07529] Component InferenceWorker_p0-w0 stopped!
[2025-02-11 08:23:23,548][07529] Waiting for process learner_proc0 to stop...
[2025-02-11 08:23:24,168][07529] Waiting for process inference_proc0-0 to join...
[2025-02-11 08:23:24,169][07529] Waiting for process rollout_proc0 to join...
[2025-02-11 08:23:24,169][07529] Waiting for process rollout_proc1 to join...
[2025-02-11 08:23:24,169][07529] Batcher 0 profile tree view:
batching: 0.0028, releasing_batches: 0.0011
[2025-02-11 08:23:24,169][07529] InferenceWorker_p0-w0 profile tree view:
wait_policy: 0.0051
  wait_policy_total: 0.3925
update_model: 0.0149
  weight_update: 0.0003
one_step: 0.0011
  handle_policy_step: 0.4362
    deserialize: 0.0180, stack: 0.0048, obs_to_device_normalize: 0.0895, forward: 0.2165, send_messages: 0.0278
    prepare_outputs: 0.0467
      to_cpu: 0.0062
[2025-02-11 08:23:24,170][07529] Learner 0 profile tree view:
misc: 0.0000, prepare_batch: 0.0355
train: 0.0307
  epoch_init: 0.0000, minibatch_init: 0.0000, losses_postprocess: 0.0003, kl_divergence: 0.0001, after_optimizer: 0.0007
  calculate_losses: 0.0082
    losses_init: 0.0000, forward_head: 0.0016, bptt_initial: 0.0000, bptt: 0.0000, tail: 0.0031, advantages_returns: 0.0005, losses: 0.0027
  update: 0.0200
    clip: 0.0017
[2025-02-11 08:23:24,170][07529] RolloutWorker_w0 profile tree view:
wait_for_trajectories: 0.0004, enqueue_policy_requests: 0.0200, env_step: 0.1483, overhead: 0.0125, complete_rollouts: 0.0006
save_policy_outputs: 0.0278
  split_output_tensors: 0.0096
[2025-02-11 08:23:24,170][07529] RolloutWorker_w1 profile tree view:
wait_for_trajectories: 0.0004, enqueue_policy_requests: 0.0271, env_step: 0.2078, overhead: 0.0176, complete_rollouts: 0.0008
save_policy_outputs: 0.0374
  split_output_tensors: 0.0120
[2025-02-11 08:23:24,170][07529] Loop Runner_EvtLoop terminating...
[2025-02-11 08:23:24,171][07529] Runner profile tree view:
main_loop: 5.0257
[2025-02-11 08:23:24,171][07529] Collected {0: 896}, FPS: 178.3
